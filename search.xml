<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>大数据离线数仓项目学习笔记</title>
    <url>/posts/hadoop/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\css\APlayer.min.css"><script src="\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\js\Meting.min.js"></script><h2 id="离线数仓项目"><a href="#离线数仓项目" class="headerlink" title="离线数仓项目"></a>离线数仓项目</h2><h3 id="一、数据仓库概念"><a href="#一、数据仓库概念" class="headerlink" title="一、数据仓库概念"></a>一、数据仓库概念</h3><h3 id="二、项目需求及架构设计"><a href="#二、项目需求及架构设计" class="headerlink" title="二、项目需求及架构设计"></a>二、项目需求及架构设计</h3><h3 id="三、数据生成模块"><a href="#三、数据生成模块" class="headerlink" title="三、数据生成模块"></a>三、数据生成模块</h3><h4 id="3-1-服务器和-JDK-准备"><a href="#3-1-服务器和-JDK-准备" class="headerlink" title="3.1 服务器和 JDK 准备"></a>3.1 服务器和 JDK 准备</h4><h5 id="3-1-1-配置IP、主机名"><a href="#3-1-1-配置IP、主机名" class="headerlink" title="3.1.1    配置IP、主机名"></a>3.1.1    配置IP、主机名</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> vim /etc/sysconfig/network-scripts/ifcfg-ens33</span></span><br><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> hostnamectl --static set-hostname hadoop102</span></span><br><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> vim /etc/hosts</span></span><br><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> vim /etc/sudoers(jajiahao  ALL=(ALL)   NOPASSWD:ALL)</span></span><br></pre></td></tr></table></figure>

<h5 id="3-1-2-编写集群分发脚本-xsync-，放到-usr-local-bin-目录下"><a href="#3-1-2-编写集群分发脚本-xsync-，放到-usr-local-bin-目录下" class="headerlink" title="3.1.2 编写集群分发脚本 xsync  ，放到 /usr/local/bin 目录下"></a>3.1.2 编写集群分发脚本 xsync  ，放到 /usr/local/bin 目录下</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=103; host&lt;105; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>修改执行权限 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> chomd 777 xsync</span></span><br></pre></td></tr></table></figure>

<h5 id="3-1-3-配置-SSH-免密登录"><a href="#3-1-3-配置-SSH-免密登录" class="headerlink" title="3.1.3 配置 SSH 免密登录"></a>3.1.3 配置 SSH 免密登录</h5><p>​    在家目录创建 .ssh 文件，进入到 .ssh 目录下，生成 公钥和私钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure>

<p>​    将 公钥 拷贝到 要免密登录的目标机器上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> ssh-copy-id hadoop102</span></span><br><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> ssh-copy-id hadoop103</span></span><br><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> ssh-copy-id hadoop104</span></span><br></pre></td></tr></table></figure>

<p>​    在 hadoop102 上采用 jajiahao 账号配置一下免密登录到 hadoop102、hadoop103、hadoop104 服务器上</p>
<p>​    在 hadoop102 上采用 root 账号配置一下免密登录到 hadoop102、hadoop103、hadoop104 服务器上</p>
<p>​    在 hadoop103 上采用 jajiahao 账号配置一下免密登录到 hadoop102、hadoop103、hadoop104 服务器上</p>
<h5 id="3-1-4-安装-JDK"><a href="#3-1-4-安装-JDK" class="headerlink" title="3.1.4 安装 JDK"></a>3.1.4 安装 JDK</h5><p>​    1）卸载现有 JDK</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 opt]$ sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps -qa | grep -i java</span><br></pre></td></tr></table></figure>

<p>​    2）将 jdk 包放入 /opt/software 目录下，解压到 /opt/module 目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span></span><br></pre></td></tr></table></figure>

<p>​    3）配置 JDK 环境变量，新建 /etc/profile.d/my_env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> sudo vim /etc/profile.d/my_env.sh</span></span><br></pre></td></tr></table></figure>

<p>​    4）添加如下内容，保存退出（:wq）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>​    5）让环境变量生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> <span class="built_in">source</span> /etc/profile.d/my_env.sh</span></span><br></pre></td></tr></table></figure>

<p>​    6）测试 JDK 是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> java -version</span></span><br></pre></td></tr></table></figure>

<p>​    7）分发 JDK</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> xsync /opt/module/jdk1.8.0_212</span></span><br></pre></td></tr></table></figure>

<p>​    8）分发 环境变量配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">[jajiahao@hadoop102]$</span><span class="bash"> sudo xsync /etc/profile.d/my_env.sh</span></span><br></pre></td></tr></table></figure>

<p>​    9）分别在hadoop103、hadoop104上执行 source</p>
<h4 id="3-2-模拟数据"><a href="#3-2-模拟数据" class="headerlink" title="3.2 模拟数据"></a>3.2 模拟数据</h4><p>​    3.2.1 将本地的 application.properties、gmall2020-mock-log-2020-04-01.jar、path2.json 上传到 hadoop102 的 /opt/module/applog 目录下</p>
<p>​    3.2.2 配置 application.properties 文件，修改业务日期</p>
<p>​    3.2.3 在 /opt/module/applog 路径下执行日志生成命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 applog]$ java -jar gmall2020-mock-log-2020-04-01.jar</span><br></pre></td></tr></table></figure>

<p>​    3.2.4 在 /home/jajiahao/bin 目录下编写集群日志生成脚本 lg.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">for i in hadoop102 hadoop103</span><br><span class="line">do</span><br><span class="line">    echo &quot;========== $i ==========&quot;</span><br><span class="line">    ssh $i &quot;cd /opt/module/applog/; java -jar gmall2020-mock-log-2020-04-01.jar &gt;/dev/null 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>​    3.2.5 修改脚本执行权限：chmod 777 lg.sh</p>
<p>​    3.2.6 在 hadoop102 和 hadoop103 下执行脚本 lg.sh ，在 /opt/module/applog/log 目录下查看生成的数据</p>
<h3 id="四、数据采集模块"><a href="#四、数据采集模块" class="headerlink" title="四、数据采集模块"></a>四、数据采集模块</h3><h4 id="4-1-集群所有进程查看脚本"><a href="#4-1-集群所有进程查看脚本" class="headerlink" title="4.1 集群所有进程查看脚本"></a>4.1 集群所有进程查看脚本</h4><p>​        1）新建脚本 xcall.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ vim xcall.sh</span><br></pre></td></tr></table></figure>

<p>​        2）脚本内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"> </span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">    echo --------- $i ----------</span><br><span class="line">    ssh $i &quot;$*&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>​        3）修改脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ chmod 777 xcall.sh</span><br></pre></td></tr></table></figure>

<p>​        4）启动脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ xcall.sh jps</span><br></pre></td></tr></table></figure>

<h4 id="4-2-Hadoop-安装部署"><a href="#4-2-Hadoop-安装部署" class="headerlink" title="4.2 Hadoop 安装部署"></a>4.2 Hadoop 安装部署</h4><h5 id="4-2-1-Hadoop安装"><a href="#4-2-1-Hadoop安装" class="headerlink" title="4.2.1 Hadoop安装"></a>4.2.1 Hadoop安装</h5><p>​        1）hadoop 3.1.3 安装 配置环境变量，将文件解压到 /opt/module 目录下，修改 my_env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>​        2）添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#HADOOP_HOME</span></span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>​        3）让配置文件生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<h5 id="4-2-2-配置集群"><a href="#4-2-2-配置集群" class="headerlink" title="4.2.2 配置集群"></a>4.2.2 配置集群</h5><p>​        1）配置核心文件：core-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd $HADOOP_HOME/etc/hadoop</span><br><span class="line">[jajiahao@hadoop102 hadoop]$ vim core-site.xml</span><br></pre></td></tr></table></figure>

<p>​            文件内容如下</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为jajiahao --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jajiahao<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置该jajiahao(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.jajiahao.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该jajiahao(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.jajiahao.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该jajiahao(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.jajiahao.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        2）HDFS 配置文件：hdfs-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop]$ vim hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p>​            文件内容如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">	<span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 测试环境指定HDFS副本的数量1 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        3）YARN 配置文件：yarn-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop]$ vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>​            文件内容如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- yarn容器允许管理的物理内存大小 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        4）MapReduce配置文件：mapred-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop]$ vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>​            文件内容如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        5）配置 workers</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop]$ vim workers</span><br></pre></td></tr></table></figure>

<p>​            内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>

<h5 id="4-2-3-配置历史服务器"><a href="#4-2-3-配置历史服务器" class="headerlink" title="4.2.3 配置历史服务器"></a>4.2.3 配置历史服务器</h5><p>​        1）配置 mapred-site.xml，添加如下内容：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="4-2-4-配置日志的聚集，配置-yarn-site-xml-，增加以下内容："><a href="#4-2-4-配置日志的聚集，配置-yarn-site-xml-，增加以下内容：" class="headerlink" title="4.2.4 配置日志的聚集，配置 yarn-site.xml ，增加以下内容："></a>4.2.4 配置日志的聚集，配置 yarn-site.xml ，增加以下内容：</h5><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="4-2-5-分发-hadoop-3-1-3-、my-env-sh-，使用-source"><a href="#4-2-5-分发-hadoop-3-1-3-、my-env-sh-，使用-source" class="headerlink" title="4.2.5 分发 hadoop-3.1.3 、my_env.sh ，使用 source"></a>4.2.5 分发 hadoop-3.1.3 、my_env.sh ，使用 source</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ xsync hadoop-3.1.3/</span><br></pre></td></tr></table></figure>

<h5 id="4-2-6-启动-hadoop-集群"><a href="#4-2-6-启动-hadoop-集群" class="headerlink" title="4.2.6 启动 hadoop 集群"></a>4.2.6 启动 hadoop 集群</h5><p>​        1）如果集群是第一次启动，需要在hadoop102节点格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[jajiahao@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure>

<p>​        2) 启动 HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>​        3) 在配置了ResourceManager的节点（hadoop103）启动YARN</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>​        4) 查看是否启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop-3.1.3]$ xcall.sh jps</span><br></pre></td></tr></table></figure>

<h5 id="4-2-7-Hadoop-群起脚本"><a href="#4-2-7-Hadoop-群起脚本" class="headerlink" title="4.2.7 Hadoop 群起脚本"></a>4.2.7 Hadoop 群起脚本</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ pwd</span><br><span class="line">/home/jajiahao/bin</span><br><span class="line">[jajiahao@hadoop102 bin]$ vim hdp.sh</span><br></pre></td></tr></table></figure>

<p>​        输入如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">    echo &quot;No Args Input...&quot;</span><br><span class="line">    exit ;</span><br><span class="line">fi</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot; =================== 启动 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 启动 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 启动 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot; =================== 关闭 hadoop集群 ===================&quot;</span><br><span class="line"></span><br><span class="line">        echo &quot; --------------- 关闭 historyserver ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 yarn ---------------&quot;</span><br><span class="line">        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span><br><span class="line">        echo &quot; --------------- 关闭 hdfs ---------------&quot;</span><br><span class="line">        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    echo &quot;Input Args Error...&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>​        修改脚本权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ chmod 777 hdp.sh</span><br></pre></td></tr></table></figure>

<p>​        测试脚本 hdp.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ hdp.sh stop</span><br><span class="line"> =================== 关闭 hadoop集群 ===================</span><br><span class="line"> --------------- 关闭 historyserver ---------------</span><br><span class="line"> --------------- 关闭 yarn ---------------</span><br><span class="line">Stopping nodemanagers</span><br><span class="line">Stopping resourcemanager</span><br><span class="line"> --------------- 关闭 hdfs ---------------</span><br><span class="line">Stopping namenodes on [hadoop102]</span><br><span class="line">Stopping datanodes</span><br><span class="line">Stopping secondary namenodes [hadoop104]</span><br><span class="line"></span><br><span class="line">[jajiahao@hadoop102 bin]$ hdp.sh start</span><br><span class="line"> =================== 启动 hadoop集群 ===================</span><br><span class="line"> --------------- 启动 hdfs ---------------</span><br><span class="line">Starting namenodes on [hadoop102]</span><br><span class="line">Starting datanodes</span><br><span class="line">Starting secondary namenodes [hadoop104]</span><br><span class="line"> --------------- 启动 yarn ---------------</span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br><span class="line"> --------------- 启动 historyserver ---------------</span><br></pre></td></tr></table></figure>

<h5 id="4-2-8-集群时间同步"><a href="#4-2-8-集群时间同步" class="headerlink" title="4.2.8 集群时间同步"></a>4.2.8 集群时间同步</h5><h5 id="4-2-9-LZO-压缩配置"><a href="#4-2-9-LZO-压缩配置" class="headerlink" title="4.2.9 LZO 压缩配置"></a>4.2.9 LZO 压缩配置</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 将jar包移动到 common 目录下</span></span><br><span class="line">[jajiahao@hadoop102 software]$ mv hadoop-lzo-0.4.20.jar /opt/module/hadoop-3.1.3/share/hadoop/common/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 分发 jar 包</span></span><br><span class="line">[jajiahao@hadoop102 common]$ xsync hadoop-lzo-0.4.20.jar</span><br></pre></td></tr></table></figure>

<p>​            1）修改 core-site.xml 增加配置支持LZO压缩</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">            org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">            org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">            org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">            org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">            com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">            com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">        <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 hadoop]$ vim core-site.xml </span><br><span class="line">[jajiahao@hadoop102 hadoop]$ xsync core-site.xml</span><br></pre></td></tr></table></figure>

<p>​                2）测试 LZO 压缩</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 测试数据准备</span></span><br><span class="line">[jajiahao@hadoop102 ~]$ hadoop fs -mkdir /input</span><br><span class="line">[jajiahao@hadoop102 hadoop-3.1.3]$ hadoop fs -put README.txt /input</span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试</span></span><br><span class="line">[jajiahao@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.codec=com.hadoop.compression.lzo.LzopCode /input /output</span><br></pre></td></tr></table></figure>

<h5 id="4-2-10-LZO-创建索引"><a href="#4-2-10-LZO-创建索引" class="headerlink" title="4.2.10  LZO 创建索引"></a>4.2.10  LZO 创建索引</h5><p>​        1）创建LZO文件的索引，LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<p>​        2）测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1. 将bigtable.lzo（200M）上传到集群的根目录</span></span><br><span class="line">[jajiahao@hadoop102 module]$ hadoop fs -put bigtable.lzo /input</span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 执行wordcount程序</span></span><br><span class="line">[jajiahao@hadoop102 module]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output</span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 对上传的LZO文件建索引</span></span><br><span class="line">[jajiahao@hadoop102 module]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo</span><br><span class="line"><span class="meta">#</span><span class="bash"> 4. 再次执行WordCount程序</span></span><br><span class="line">[jajiahao@hadoop102 module]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input /output2</span><br></pre></td></tr></table></figure>

<h5 id="4-2-11-基准测试"><a href="#4-2-11-基准测试" class="headerlink" title="4.2.11 基准测试"></a>4.2.11 基准测试</h5><p>​        1）测试 HDFS 写性能</p>
<p>​            测试内容：向 HDFS 集群写 7（CPU核数 -1 ）个128M的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 7 -fileSize 128MB</span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果</span></span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:             Date &amp; time: Wed Nov 04 13:18:25 CST 2020</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:         Number of files: 7</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:  Total MBytes processed: 896</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:       Throughput mb/sec: 44.51</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:  Average IO rate mb/sec: 92.26</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:   IO rate std deviation: 85.19</span><br><span class="line">2020-11-04 13:18:25,002 INFO fs.TestDFSIO:      Test exec time sec: 22.63</span><br></pre></td></tr></table></figure>

<p>​    注意：</p>
<p>​        Number of files：生成mapTask数量，一般是集群中CPU核数-1，我们测试虚拟机就按照实际的物理内存-1分配即可</p>
<p>​        Total MBytes processed：单个map处理的文件大小</p>
<p>​        Throughput mb/sec  ：单个mapTak的吞吐量    </p>
<p>​                计算方式：处理的总文件大小/每一个mapTask写数据的时间累加</p>
<p>​        集群整体吞吐量：生成mapTask数量*单个mapTak的吞吐量</p>
<p>​        Average IO rate mb/sec：单个mapTak的吞吐量  </p>
<p>​                计算方式：每个mapTask处理文件大小/每一个mapTask写数据的时间 累加/生成mapTask数量</p>
<p>​        IO rate std deviation：方差、反映各个mapTask处理的差值，越小越均衡</p>
<p>​        2）测试 HDFS 读性能</p>
<p>​                测试内容：读取 HDFS 集群7个128M的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 7 -fileSize 128MB</span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果</span></span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:             Date &amp; time: Wed Nov 04 13:27:31 CST 2020</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:         Number of files: 7</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:  Total MBytes processed: 896</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:       Throughput mb/sec: 289.31</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:  Average IO rate mb/sec: 415.26</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:   IO rate std deviation: 289.65</span><br><span class="line">2020-11-04 13:27:31,475 INFO fs.TestDFSIO:      Test exec time sec: 17.13</span><br></pre></td></tr></table></figure>

<p>​        3）删除测试生成数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean</span><br></pre></td></tr></table></figure>

<h4 id="4-3-Zookeeper-安装"><a href="#4-3-Zookeeper-安装" class="headerlink" title="4.3 Zookeeper 安装"></a>4.3 Zookeeper 安装</h4><h5 id="4-3-1-集群规划"><a href="#4-3-1-集群规划" class="headerlink" title="4.3.1 集群规划"></a>4.3.1 集群规划</h5><p>​            在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper</p>
<h5 id="4-3-2-解压安装"><a href="#4-3-2-解压安装" class="headerlink" title="4.3.2 解压安装"></a>4.3.2 解压安装</h5><p>​            1）解压 Zookeeper安装包到 /opt/module/ 目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 software]$ tar -zxvf zookeeper-3.5.7.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>​            2）修改 /opt/module/apache-zookeeper-3.5.7-bin名称为zookeeper-3.5.7</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7</span><br></pre></td></tr></table></figure>

<p>​            3）分发 /opt/module/zookeeper-3.5.7 目录内容到 hadoop103、hadoop104</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ xsync zookeeper-3.5.7/</span><br></pre></td></tr></table></figure>

<h5 id="4-3-3-ZK集群启动停止脚本"><a href="#4-3-3-ZK集群启动停止脚本" class="headerlink" title="4.3.3 ZK集群启动停止脚本"></a>4.3.3 ZK集群启动停止脚本</h5><p>​        1）在hadoop102的/home/atguigu/bin目录下创建脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ vim zk.sh</span><br></pre></td></tr></table></figure>

<p>​            脚本内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">	for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">	do</span><br><span class="line">        echo ---------- zookeeper $i 启动 ------------</span><br><span class="line">		ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh start&quot;</span><br><span class="line">	done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">	for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">	do</span><br><span class="line">        echo ---------- zookeeper $i 停止 ------------    </span><br><span class="line">		ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop&quot;</span><br><span class="line">	done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">	for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">	do</span><br><span class="line">        echo ---------- zookeeper $i 状态 ------------    </span><br><span class="line">		ssh $i &quot;/opt/module/zookeeper-3.5.7/bin/zkServer.sh status&quot;</span><br><span class="line">	done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>​        2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ chmod 777 zk.sh </span><br></pre></td></tr></table></figure>

<p>​        3）脚本测试：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看状态</span></span><br><span class="line">[jajiahao@hadoop102 bin]$ zk.sh status</span><br><span class="line">---------- zookeeper hadoop102 状态 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">---------- zookeeper hadoop103 状态 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader</span><br><span class="line">---------- zookeeper hadoop104 状态 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 集群停止</span></span><br><span class="line">[jajiahao@hadoop102 bin]$ zk.sh stop</span><br><span class="line">---------- zookeeper hadoop102 停止 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">---------- zookeeper hadoop103 停止 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">---------- zookeeper hadoop104 停止 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 集群启动</span></span><br><span class="line">[jajiahao@hadoop102 bin]$ zk.sh start</span><br><span class="line">---------- zookeeper hadoop102 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">---------- zookeeper hadoop103 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">---------- zookeeper hadoop104 启动 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure>

<h4 id="4-4-Kafka-安装部署"><a href="#4-4-Kafka-安装部署" class="headerlink" title="4.4 Kafka 安装部署"></a>4.4 Kafka 安装部署</h4><p>​        1）解压安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 software]$ tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>​        2）修改解压后的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ mv kafka_2.11-2.4.1/ kafka</span><br></pre></td></tr></table></figure>

<p>​        3）在 /opt/module/kafka 目录下创建 logs 文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ cd kafka/</span><br><span class="line">[jajiahao@hadoop102 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure>

<p>​        4）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 kafka]$ cd config/</span><br><span class="line">[jajiahao@hadoop102 config]$ vim server.properties</span><br></pre></td></tr></table></figure>

<p>​            修改或者增加以下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> broker 的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除topic功能使能</span></span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka运行日志存放的路径</span></span><br><span class="line">log.dirs=/opt/module/kafka/data</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br></pre></td></tr></table></figure>

<p>​        5）配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>​            添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>​            让环境变量生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ source /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>​        6）分发安装包（注意：分发之后记得配置其他机器的环境变量）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ xsync kafka/</span><br></pre></td></tr></table></figure>

<p>​        7）分别在hadoop103和hadoop104上修改配置文件 /opt/module/kafka/config/server.properties 中的 broker.id=1、broker.id=2</p>
<p>​        8）启动集群：依次在hadoop102、hadoop103、hadoop104节点上启动 kafka</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties</span><br><span class="line">[jajiahao@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties</span><br><span class="line">[jajiahao@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties</span><br></pre></td></tr></table></figure>

<p>​        9）关闭集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[jajiahao@hadoop103 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[jajiahao@hadoop104 kafka]$ bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>​        10）新建群起脚本，在 /home/jajiahao/bin 目录下创建脚本 kf.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ vim kf.sh</span><br></pre></td></tr></table></figure>

<p>​            脚本内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------启动 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties &quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">    do</span><br><span class="line">        echo &quot; --------停止 $i Kafka-------&quot;</span><br><span class="line">        ssh $i &quot;/opt/module/kafka/bin/kafka-server-stop.sh stop&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>​        11）修改脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ chmod 777 kf.sh</span><br></pre></td></tr></table></figure>

<p>​        12）脚本测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kf集群脚本启动</span></span><br><span class="line">[jajiahao@hadoop102 module]$ kf.sh start</span><br><span class="line"><span class="meta">#</span><span class="bash"> kf集群脚本停止</span></span><br><span class="line">[jajiahao@hadoop102 module]$ kf.sh stop</span><br></pre></td></tr></table></figure>

<p>​        13）Kafka 压力测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 kafka]$ bin/kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果</span></span><br><span class="line">100000 records sent, 40933.278756 records/sec (3.90 MB/sec), 593.53 ms avg latency, 820.00 ms max latency, 633 ms 50th, 807 ms 95th, 817 ms 99th, 819 ms 99.9th.</span><br></pre></td></tr></table></figure>

<p>​        14）Kafka Consumer 压力测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 kafka]$ bin/kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果</span></span><br><span class="line">2020-11-09 14:32:23:001, 2020-11-09 14:32:35:193, 9.5367, 0.7822, 100000, 8202.0997, 1604903543892, -1604903531700, -0.0000, -0.0001</span><br></pre></td></tr></table></figure>

<p>​            测试结果说明：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec</span><br><span class="line">开始测试时间，测试结束时间，共消费数据9.5367MB，吞吐量0.7822MB/s，共消费100000条，平均每秒消费8202.0997条，</span><br><span class="line">再平衡时间(ms)，fetch平均时间(ms)，fetch平均大小(MB/s)，fetch平均条数(条)</span><br></pre></td></tr></table></figure>

<h4 id="4-5-Flume-安装配置"><a href="#4-5-Flume-安装配置" class="headerlink" title="4.5  Flume 安装配置"></a>4.5  Flume 安装配置</h4><h5 id="4-5-1-Flume-安装"><a href="#4-5-1-Flume-安装" class="headerlink" title="4.5.1 Flume 安装"></a>4.5.1 Flume 安装</h5><p>​        1） 将 apache-flume-1.9.0-bin.tar.gz 上传到 linux 的 /opt/software 目录下</p>
<p>​        2）解压apache-flume-1.9.0-bin.tar.gz到/opt/module/目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 software]$ tar -zxvf apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>​        3） 修改 apache-flume-1.9.0-bin 的名称为 flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ mv apache-flume-1.9.0-bin/ flume</span><br></pre></td></tr></table></figure>

<p>​        4） 将lib文件夹下的guava-11.0.2.jar删除或修改以兼容Hadoop 3.1.3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 lib]$ mv guava-11.0.2.jar guava-11.0.2.jar.bak</span><br><span class="line">[jajiahao@hadoop102 lib]$ rm -rf guava-11.0.2.jar.bak</span><br></pre></td></tr></table></figure>

<p>​        5） 将 flume/conf 下的 flume-env.sh.template 文件修改为 flume-env.sh，并配置 flume-env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[jajiahao@hadoop102 conf]$ vim flume-env.sh</span><br></pre></td></tr></table></figure>

<p>​            添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<p>​        6） 分发flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ xsync flume/</span><br></pre></td></tr></table></figure>

<h5 id="4-5-2-Flume-配置"><a href="#4-5-2-Flume-配置" class="headerlink" title="4.5.2 Flume 配置"></a>4.5.2 Flume 配置</h5><p>​        1） 在 /opt/module/flume/conf 目录下创建 file-flume-kafka.conf 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 conf]$ vim file_flume_kafka.conf</span><br></pre></td></tr></table></figure>

<p>​            在文件配置如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 为各组件命名</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 描述<span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json</span><br><span class="line">a1.sources.r1.interceptors =  i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.jajiahao.flume.interceptor.ETLInterceptor$Builder</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 描述channel</span></span><br><span class="line">a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092</span><br><span class="line">a1.channels.c1.kafka.topic = topic_log</span><br><span class="line">a1.channels.c1.parseAsFlumeEvent = false</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 绑定<span class="built_in">source</span>和channel以及sink和channel的关系</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<p>​            注：com.atguigu.flume.interceptor.ETLInterceptor是自定义的拦截器的全类名，需要根据用户自定义的拦截器做相应修改</p>
<h5 id="4-5-3-Flume-拦截器"><a href="#4-5-3-Flume-拦截器" class="headerlink" title="4.5.3 Flume 拦截器"></a>4.5.3 Flume 拦截器</h5><p>​        1）创建Maven工程flume-interceptor</p>
<p>​        2) 创建包名：com.atguigu.flume.interceptor</p>
<p>​        3) 在pom.xml 文件中添加如下配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.62<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        4） 在com.atguigu.flume.interceptor包下创建 JSONUtils 类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.jajiahao.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JSONUtils</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isValidate</span><span class="params">(String log)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            JSON.parse(log);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;<span class="keyword">catch</span>(JSONException e)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​        5）在 com.atguigu.flume.interceptor 包下创建 ETLInterceptor 类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.jajiahao.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 获取数据</span></span><br><span class="line">        <span class="keyword">byte</span>[] body = event.getBody();</span><br><span class="line">        String log = <span class="keyword">new</span> String(body, StandardCharsets.UTF_8);</span><br><span class="line">        <span class="comment">// 2. 校验</span></span><br><span class="line">        <span class="keyword">if</span>(JSONUtils.isValidate(log))&#123;</span><br><span class="line">            <span class="keyword">return</span> event;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; list)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Iterator&lt;Event&gt; iterator = list.iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">            Event next = iterator.next();</span><br><span class="line">            <span class="keyword">if</span>(intercept(next)==<span class="keyword">null</span>)&#123;</span><br><span class="line">                iterator.remove();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> ETLInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>​        6）打包得到 flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar 文件</p>
<p>​        7）将打好的包放入到 hadoop102 的 /opt/module/flume/lib 目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 lib]$ ls | grep inter</span><br><span class="line">flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<p>​        8）分发 jar包 到hadoop103、hadoop104</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 lib]$ xsync flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>

<h5 id="4-5-4-Flume-启动停止脚本"><a href="#4-5-4-Flume-启动停止脚本" class="headerlink" title="4.5.4 Flume 启动停止脚本"></a>4.5.4 Flume 启动停止脚本</h5><p>​        1）在/home/atguigu/bin目录下创建脚本 f1.sh</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ vim f1.sh</span><br></pre></td></tr></table></figure>

<p>​            在脚本中填写如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103</span><br><span class="line">        do</span><br><span class="line">                echo &quot; --------启动 $i 采集flume-------&quot;</span><br><span class="line">                ssh $i &quot;nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE &gt;/opt/module/flume/log1.txt 2&gt;&amp;1  &amp;&quot;</span><br><span class="line">        done</span><br><span class="line">&#125;;;	</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">        for i in hadoop102 hadoop103</span><br><span class="line">        do</span><br><span class="line">                echo &quot; --------停止 $i 采集flume-------&quot;</span><br><span class="line">                ssh $i &quot;ps -ef | grep file-flume-kafka | grep -v grep |awk  &#x27;&#123;print \$2&#125;&#x27; | xargs -n1 kill -9 &quot;</span><br><span class="line">        done</span><br><span class="line"></span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>​        2）增加脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 bin]$ chmod 777 f1.sh </span><br></pre></td></tr></table></figure>

<p>​        3）f1 集群启动脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ f1.sh start</span><br><span class="line"> --------启动 hadoop102 采集flume-------</span><br><span class="line"> --------启动 hadoop103 采集flume-------</span><br></pre></td></tr></table></figure>

<p>​        4）f1 集群停止脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[jajiahao@hadoop102 module]$ f1.sh stop</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
        <category>数仓项目</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Java</tag>
        <tag>数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>会计小白入门学习笔记</title>
    <url>/posts/cpa/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\css\APlayer.min.css"><script src="\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\js\Meting.min.js"></script><p><img src="https://cdn.jsdelivr.net/gh/JajiaHao/image/cpa/kj-1.png"></p>
]]></content>
      <categories>
        <category>CPA</category>
        <category>会计</category>
      </categories>
      <tags>
        <tag>会计</tag>
        <tag>CPA</tag>
      </tags>
  </entry>
</search>
